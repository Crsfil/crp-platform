# Легенда: CRP-платформа (Газпромбанк Лизинг)

## Короткий питч
- 2 года в AGIMA: старт джуном (8 мес), далее миддл.
- 2.5 года в Газпромбанк Лизинг на позиции миддл Java backend.
- Домен: лизинг строительной и грузовой техники; собственная CRP на микросервисах.
- Стек: Java 17, Spring Boot/Web/Security, Hibernate/JPA, PostgreSQL, Liquibase, JUnit, Maven, Swagger (springdoc), Lombok, Git, Kubernetes.

## Команда
- Состав: тимлид (backend), 3 backend, 1 frontend, 1 QA, 1 аналитик, 1 DevOps.
- Формат: канбан-доска с двухнедельным рабочим тактом; созвоны и релизы синхронизированы на 2-недельном ритме.

## Процессы (Kanban + 2-недельный такт)
- Доска и статусы колонок:
  - Backlog — приоритизированные задачи; команда вытягивает верхние.
  - Next — отобранные на следующий цикл анализа/работы.
  - Analysis — анализ требований и проектирование.
  - Dev Ready — готово в разработку.
  - In Progress — в работе у разработчиков.
  - Dev Done — готово к тестированию.
  - Test — тестирование; при баге QA ставит флаг блокировки, разработчик фиксит и снимает блок.
  - Deploy — готово к выкладке в прод.
  - Done — полностью завершено, не требует доработок.
- Target Date: у каждой задачи задан срок. За 2 дня до дедлайна задача подсвечивается красным; при пропуске становится серой (учёт как техдолг). Раз в неделю выделяем слот на устранение техдолга.
- Доп. статусы задач: новая, в работе, блокировка.
- Церемонии:
  - Ежедневные дейлики: синхронизация команды с владельцем продукта и лидом.
  - Ежемесячные ретроспективы: что улучшить в процессе/качестве.
  - Груминг backlog: изучение, переоценка, приоритезация задач.
  - Внутренние митапы: обмен опытом и техфишками.

## Жизненный цикл разработки
1) Анализ бизнеса заказчика и сбор требований.
2) Техническая документация: функции системы, архитектура, сроки/стоимость.
3) Прототипирование, пилотное внедрение, сбор обратной связи и новых требований.
4) Разработка с учётом замечаний, финальное внедрение у клиента.
5) Исправление выявленных недочётов с точки зрения клиента.
6) Дальнейшее сопровождение и развитие согласно потребностям.

## Проекты
- CRM для кредитных специалистов — поддержка и доработки backend (кратко упомянуть).
- CRP-платформа для лизинга техники:
  - Предпосылки: отказ от MS Dynamics; отечественные альтернативы не покрыли требования по качеству/стоимости → решение разработать собственную платформу.
  - Архитектура: микросервисы по доменам (клиенты/контрагенты, активы, заявки, расчёт прайсинга и графиков, платежи, документы, уведомления, интеграции).
  - Коммуникации: REST, контракты документируются через OpenAPI/Swagger.
  - Данные: отдельные схемы PostgreSQL на сервис; миграции Liquibase (строгий порядок, проверки checksum, rollback-скрипты).
  - Деплой и окружения: Kubernetes (readiness/liveness, RollingUpdate), профили dev/stage/prod.

## Архитектура безопасности и аутентификации

Проект прошёл через несколько этапов зрелости в части аутентификации и авторизации, что привело к созданию унифицированного и безопасного стека на базе OIDC.

### Ключевые компоненты и решения

-   **Keycloak (IdP)**: Развёрнут корпоративный Identity Provider. Для фронтенда и BFF используется публичный клиент, для межсервисного взаимодействия (S2S) — конфиденциальные клиенты. Настроены протокольные мапперы, включая `audience-mapper` для корректной доставки `aud` в токене. Ротация ключей обеспечивается стандартным механизмом JWKS (`/.well-known/jwks.json`).
-   **common-security**: Общий модуль, подключаемый во все микросервисы. Содержит автоконфигурацию для Spring Security, которая создаёт `JwtDecoder` на основе `issuer-uri` из конфига. Модуль принудительно включает валидацию эмитента (`iss`) и аудитории (`aud`). Ожидаемая аудитория по умолчанию — `spring.application.name`.
-   **service-auth-client**: Клиентская библиотека для межсервисных вызовов (S2S). Включает `ClientCredentialsTokenManager` для получения и кэширования токенов по `grant_type=client_credentials` и `BearerExchangeFilter` — фильтр для `WebClient`, который автоматически добавляет заголовок `Authorization: Bearer` в исходящие запросы.
-   **BFF (Gateway)**: На уровне шлюза реализована логика обновления пользовательских сессий. Используется `refresh_token` для получения новой пары токенов. Для защиты от "шторма" обновлений (когда множество параллельных запросов от клиента инициируют многократное обновление токена) реализован механизм `single-flight` с координацией через Redis.

### Хронология миграции

1.  **Исходное состояние (MVP)**: Часть сервисов использовала HMAC (HS256) с общим секретом, другая — самописные JWT без строгой валидации. Это создавало риски безопасности, так как токен, предназначенный для одного сервиса, мог быть принят другим.
2.  **Переход на единый JWT**: Новые сервисы (inventory, procurement, reports) сразу разрабатывались с использованием Spring Security Resource Server и валидацией `iss`.
3.  **Внедрение строгой проверки аудитории (`aud`)**: С появлением `common-security` все сервисы начали проверять, что `aud` в токене соответствует их `spring.application.name`. Это исключило возможность "переноса" токена между сервисами.
4.  **Централизация обновления токенов**: Логика `refresh_token` была вынесена на шлюз (BFF), что упростило бэкенд-сервисы — они стали принимать только валидный `access_token`.
5.  **Интеграция с Keycloak**: Все сервисы были перенастроены на `issuer-uri` корпоративного Keycloak. Это позволило централизованно управлять клиентами, ролями и ключами (через JWKS).
6.  **Безопасное межсервисное взаимодействие (S2S)**: Вместо статического `X-Internal-API-Key` был внедрён `service-auth-client`. Теперь для S2S-вызовов используются JWT, полученные через `client_credentials`, со строгой аудиторией, настроенной в Keycloak.

Этот подход позволил унифицировать безопасность, снизить конфигурационный долг и повысить общий уровень защищённости платформы.

## Роль и развитие
- Старт (на подхвате): модульные/интеграционные тесты, багфиксы, поддержка Swagger и Liquibase, подготовка техдоков.
- Далее (микросервисы «под ключ»): от дизайна схемы БД и API до прод-релизов и сопровождения.
- Участие: оценки на грумингах, код-ревью, ведение техзадач по качеству.

## Зона ответственности
- REST-API: Spring Web, валидация, DTO/маппинг, единый формат ошибок.
- Бизнес-логика: расчёт графиков, комиссий, штрафов, правила состояний заявок.
- Доступы: Spring Security + JWT, роли/права, `@PreAuthorize` на методах.
- JPA/Hibernate: модели/связи, борьба с N+1 (`join fetch`, `EntityGraph`), аудит, оптимистические блокировки (`@Version`).
- PostgreSQL: индексы (композитные/частичные), EXPLAIN ANALYZE, JSONB для гибких атрибутов.
- Миграции: Liquibase changelog per service, валидация и rollback-план.
- Тестирование: JUnit, `@SpringBootTest`/`@WebMvcTest`, моки внешних интеграций.
- Деплой: взаимодействие с DevOps, ресурсы/лимиты, наблюдаемость (readiness/liveness, метрики).

## Технологии (как применял)
- Java 17: `record` для DTO, enhanced switch, Stream API.
- Spring Boot: `application-*`, `@ConfigurationProperties`, профили, actuator.
- Spring Security: JWT-фильтр, Role/Authority, маскирование чувствительных полей в ответах.
- Hibernate/JPA: каскады, LAZY/Fetch стратегии, батчевые операции.
- PostgreSQL: нормализация, индексация, материализованные представления (где нужно).
- Liquibase: строгое версионирование, изолированные changeSet’ы, миграции через CI.
- Maven: multimodule, BOM, surefire/failsafe, jacoco.
- Swagger: springdoc-openapi, автогенерация схем и UI.
- K8s: манифесты, стратегии обновлений, health checks.

## Сложные кейсы (STAR)
- Миграция с MS Dynamics
  - S: Нужна собственная платформа, перенос сущностей: клиенты, сделки, документы.
  - T: Спроектировать модели, безопасно перенести данные, не останавливая бизнес.
  - A: Согласовал маппинг с аналитиками; подготовил Liquibase-схемы; валидаторы и скрипты сверки; пилотный прогон на стенде.
  - R: Перенос без простоя; снизили TCO, ускорили последующие доработки.

- Производительность поиска/отчётов
  - S: Медленные запросы по заявкам/контрагентам на проде.
  - T: Ускорить ответы и снизить нагрузку на БД.
  - A: EXPLAIN ANALYZE, композитные/частичные индексы, исправление пагинации, устранение N+1, вынесение тяжёлых агрегаций в отдельные эндпоинты.
  - R: Ключевые запросы ускорены, пики нагрузки стабилизированы.

- Конкурентные обновления заявок
  - S: Конфликты между менеджером и риск-офицером при параллельных правках.
  - T: Исключить порчу данных, обеспечить предсказуемые ошибки.
  - A: Оптимистические блокировки (`@Version`), идемпотентные операции, бизнес-правила переходов состояний.
  - R: Конфликты исчезли; при гонке — корректный 409 с понятным описанием.

- Унификация и усиление безопасности (OIDC/JWT)
  - S: Платформа использовала смешанные подходы к аутентификации (HMAC, самописные JWT), что создавало риски (перенос токена) и усложняло поддержку. Требовалось внедрить единый, безопасный и масштабируемый стандарт.
  - T: Спроектировать и реализовать переход на OIDC-совместимый стек с асимметричной подписью (RS256), строгой проверкой токенов (iss, aud) и централизованным управлением доступом.
  - A: Развернул Keycloak как IdP. Создал модуль `common-security` для автоконфигурации валидации JWT во всех сервисах. Внедрил строгую проверку аудитории (`aud`), равной имени сервиса. Перевёл S2S-вызовы с API-ключей на `client_credentials` с помощью `service-auth-client`. Убрал устаревшие HMAC-секреты и конфигурации.
  - R: Внедрён единый стандарт аутентификации на всей платформе. Исключены риски, связанные с переносом токенов. Упростилось добавление новых сервисов и настройка доступов. Пройдена внутренняя проверка безопасности без замечаний к архитектуре.

- Защита от "шторма" обновлений токенов в BFF
  - S: В архитектуре BFF (Backend For Frontend) клиент мог инициировать несколько одновременных запросов в момент истечения `access_token`. Каждый запрос приводил к попытке обновления токена через `refresh_token`, создавая лишнюю нагрузку на IdP и риски race condition.
  - T: Реализовать механизм, гарантирующий, что для одной пользовательской сессии в один момент времени выполняется только один запрос на обновление токена.
  - A: Спроектировал и реализовал `single-flight` паттерн с использованием Redis для распределённой блокировки. Первый запрос захватывает лок (`SETNX`), выполняет обновление и публикует результат в Redis. Остальные запросы в это время ожидают публикации результата. Добавил метрики для мониторинга (количество локов, ожиданий, ошибок).
  - R: Нагрузка на IdP в моменты обновления токенов снизилась до одного запроса на сессию. Исключены ошибки, связанные с конкурентным обновлением. Механизм работает стабильно в проде при пиковых нагрузках.

- Безопасное межсервисное взаимодействие (S2S)
  - S: Для внутренних вызовов между микросервисами использовался статический заголовок `X-Internal-API-Key`, что было небезопасно (один ключ на все сервисы) и негибко.
  - T: Заменить API-ключ на современный механизм аутентификации, основанный на OAuth 2.0, с гранулярным контролем доступа.
  - A: Разработал библиотеку `service-auth-client`, использующую `grant_type=client_credentials` для получения токенов. В Keycloak завёл сервисные аккаунты (конфиденциальные клиенты) для каждой пары "вызывающий-целевой". С помощью `audience-mapper` настроил строгую привязку токена к целевому сервису.
  - R: Полностью отказались от `X-Internal-API-Key`. Каждый межсервисный вызов теперь аутентифицирован и авторизован с помощью JWT с короткоживущим токеном и строгой аудиторией, что значительно повысило безопасность и наблюдаемость системы.

## Достижения
- Вёл несколько микросервисов от схемы до прод-релизов.
- Систематически снижал техдолг: N+1, порядок миграций, единый формат ошибок.
- Улучшил DX: гайдлайны для API/Swagger/DTO, шаблоны ответов.

## Про процессы: готовые ответы
- Почему Kanban, но 2-недельный ритм? — Используем канбан-доску для вытягивания и визуализации потока, а релизы и синхронизации выстроены в 2-недельном такте — это баланс предсказуемости и гибкости.
- Как контролируете сроки? — Target Date + визуальные сигналы (красный за 2 дня, серый после пропуска), еженедельные слоты на техдолг, лимит WIP на разработку/тест.
- Что на дейликах? — Прогресс, блокеры, план до следующего дейлика; присутствуют владелец продукта и лид.
- Как работает тестовая колонка? — QA ставит флаг при баге; задача помечается как блокировка, разработчик фиксит, снимает блок, тестирование продолжается.

## Примечания
- При необходимости добавлю безопасные метрики (диапазоны) для lead time/cycle time, процент прохождения тестов, долю техдолга — уточнить перед собеседованием.

## Скрипт для собеседования (2–3 минуты)
- Начало: 2 года в AGIMA (джун → миддл), затем 2.5 года в Газпромбанк Лизинг. Команда 7 человек, канбан с двухнедельным ритмом. Стек: Java 17, Spring Boot, PostgreSQL, Liquibase, K8s.
- Середина: вел микросервисы «под ключ»: API, бизнес-логика, миграции БД, безопасность, интеграции. Сфокусирован на качестве данных и производительности.
- Dev: DoR/DoD, ветка `feature/*`, Liquibase, код-ревью, поэтапный деплой, мониторинг после релиза.
- Тесты: JUnit5 + Mockito + Testcontainers; `@WebMvcTest` и интеграционные `@SpringBootTest`; покрытие ≥ 80% (целюсь в 90%). Баг — значит автотест в регрессию.
- Финал: кратко отмечаю вклад (устранил N+1, оформил гайдлайны API/ошибок) и предлагаю обсудить один из «интересных кейсов» ниже.

## Кейсы для созвона (коротко, 2–3 мин каждый)
- Миграция с MS Dynamics: маппинг сущностей, Liquibase-план, пилот без простоя, сверка данных.
- Ускорение поиска/отчётов: EXPLAIN, индексы, устранение N+1, правильная пагинация, вынос тяжёлых агрегаций.
- Конкурентные правки заявок: оптимистические блокировки `@Version`, идемпотентность, правила состояний, предсказуемые 409.
- RBAC и безопасность: JWT, `@PreAuthorize`, маскирование чувствительных полей, прохождение внутреннего аудита.
- Окружения и тесты: Linux + Docker Compose, Testcontainers, дампы обезличенных данных, качественные фикстуры.

## Среды и окружения
- DEV на рабочих станциях Linux. Вся разработка ведётся в среде, максимально приближенной к PROD.
- Docker Compose для локального поднятия зависимостей:
  - `PostgreSQL` в контейнере, инициализация тестовыми данными из обезличенного бэкапа.
  - Локальные сервисы поднимаются рядом/или в compose, соответствуют версиям библиотек и переменным окружения PROD.
- Testcontainers для автотестов: на каждый прогон поднимается изолированный `PostgreSQL` с миграциями Liquibase и загрузкой минимально необходимого фикстурного набора.
- Удалённый тестовый сервер: после прогона локальных контейнеров возможна проверка на удалённой тестовой БД с тем же дампом, что и локально (валидация миграций и совместимости данных).

### Быстрые команды (пример)
- Запуск БД локально: `docker compose -f docker-compose.dev.yml up -d db`
- Прогон миграций локально: `mvn -Pdev liquibase:update`
- Запуск всех сервисов: `docker compose -f docker-compose.dev.yml up -d`

## Процесс разработки (E2E)
- Инициирование: задача попадает в Backlog с бизнес-контекстом, целями, критериями приёмки (AC) и target date.
- Груминг: декомпозиция, выявление зависимостей/рисков, предварительная оценка, уточнение AC.
- Analysis: проектирование API (OpenAPI), схем БД, миграций Liquibase, ролей/прав, негативных сценариев. Результат — артефакты и план проверок.
- Dev Ready (DoR выполнен): есть согласованные AC, черновик контракта API/схемы, план тестов, стратегия релиза/фича-флаг.
- In Progress: разработка в ветке `feature/{ключ-задачи-коротко}`; реализация доменной логики, контроллеров, репозиториев, валидации, безопасности; миграции Liquibase; обновление OpenAPI.
- Self-check: локальный прогон сборки и тестов, миграции применяются чисто, ручной смоук на локальном/DEV.
- Dev Done: оформлен PR, добавлена документация (README/Swagger), чек-лист самопроверки пройден.
- Test: QA гоняет тест-кейсы; баги возвращаются в разработку (статус «блокировка»), после фикса — снова в Test.
- Deploy: промо на DEV/STAGE, затем на PROD; миграции применяются перед выкладкой приложения; мониторинг логов/метрик после релиза.
- Done: UAT/приёмка от владельца продукта; релиз-заметки; задачи по фоллоу-апу в Backlog.

## Definition of Ready / Definition of Done
- DoR (готово в разработку):
  - Описаны цель и AC; понятны роли/права; определены границы; нет блокирующих зависимостей.
  - Есть черновик API/контракта и схема данных; продуман план миграций.
  - Определена стратегия тестирования и риски; оценка согласована.
- DoD (готово к завершению):
  - Все тесты зелёные; миграции применяются и откатываются; нет критических замечаний ревью.
  - Обновлены Swagger/документация; фича защищена ролями; обработка ошибок единообразна.
  - Деплой на целевое окружение прошёл; мониторинг без регрессий; приёмка от PO получена.

## Тестирование: стратегия и практики
- Пирамида тестов: юнит → интеграционные → смоук/E2E. Основной упор — юнит и интеграционные.
- Юнит-тесты (JUnit 5): изолируем бизнес-логику, чёткие Given/When/Then, проверяем граничные случаи (округления, идемпотентность, валидация).
- Mockito: мокаем внешние зависимости и интеграции; стабы на внешние API.
- Web-слой: `@WebMvcTest`/MockMvc — проверка валидаций, сериализации, кодов 2xx/4xx/5xx, правил безопасности (401/403).
- Интеграционные: `@SpringBootTest` — полный контекст, реальные репозитории, транзакции; применяем Liquibase перед тестами; проверяем запросы/индексы через EXPLAIN при необходимости.
- База и миграции: каждый changeSet имеет forward/rollback; прогоняем «чистое» и «инкрементальное» применение; проверяем совместимость со старыми данными.
- Контракты API: спецификация OpenAPI синхронизирована с контроллерами; нераскрытые поля и ошибки документированы.
- Безопасность: позитив/негатив по ролям и правам, маскирование чувствительных полей в ответах.
- Производительность: на узких местах — профилирование запросов, контроль N+1, корректная пагинация.
- E2E/смоук: ключевые бизнес-потоки (создание заявки → расчёт → одобрение → выпуск договора) на DEV/STAGE.
- Регрессии: на каждый найденный баг добавляем автотест, воспроизводящий сценарий, затем фикс.
- Тест-данные: фабрики/фикстуры, детерминированные идентификаторы, изоляция тестов, очистка после прогона.
- Quality Gates: сборка успешна, тесты зелёные и не флаки; статические проверки пройдены; PR одобрен; миграции валидны.

### Покрытие и правила приёмки
- JaCoCo: минимальный порог покрытия модульного кода — 80%; целевой — 90% и выше.
- Фича без юнит-тестов или с покрытием ниже порога не считается выполненной (не проходит DoD/quality gate).
- Исключения из порога — только по согласованию с тимлидом и явной мотивацией (генерируемый код, адаптеры/DTO без логики и т. п.).


## Code Review и Git-поток
- Ветки: `feature/*`, `bugfix/*`, `hotfix/*`; из `develop` или `main` по политике репо.
- PR: небольшой объём, осмысленные коммиты, описание изменений и рисков, чек-лист DoD.
- Ревью: минимум один ревьюер; проверяем читаемость, инварианты домена, ошибки/валидации, SQL.
- Слияние: squash/merge по договорённости; теги релизов; релиз-ноты.
